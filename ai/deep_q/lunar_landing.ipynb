{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /home/codespace/.python/current/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/codespace/.local/lib/python3.12/site-packages (from gymnasium) (2.2.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /home/codespace/.python/current/lib/python3.12/site-packages (1.0.0)\n",
      "\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /home/codespace/.local/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (2.2.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
      "Requirement already satisfied: ale-py>=0.9 in /home/codespace/.python/current/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (0.10.1)\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Requirement already satisfied: gymnasium[box2d] in /home/codespace/.python/current/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/codespace/.local/lib/python3.12/site-packages (from gymnasium[box2d]) (2.2.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from gymnasium[box2d]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from gymnasium[box2d]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /home/codespace/.python/current/lib/python3.12/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in /home/codespace/.python/current/lib/python3.12/site-packages (from gymnasium[box2d]) (4.3.0)\n"
     ]
    }
   ],
   "source": [
    "# install gymnasium\n",
    "!pip install gymnasium\n",
    "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "!apt-get install -y swig\n",
    "!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed=42):\n",
    "        super(Network, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    # forward method to next layer\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (8,)\n",
      "State size: 8\n",
      "Number of Actions: 4\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('LunarLander-v3')\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = env.observation_space.shape[0]\n",
    "no_actions = env.action_space.n\n",
    "print(f'State shape: {state_shape}')\n",
    "print(f'State size: {state_size}')\n",
    "print(f'Number of Actions: {no_actions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4\n",
    "minibatch_size = 100\n",
    "discount_factor = .99\n",
    "replay_buffer_size = int(2e4) # Remember to change this to int(1e5) when run on Google Colab\n",
    "interpolation_param = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, event):\n",
    "        '''\n",
    "        Add Experiences to Replay Memory Buffer.\n",
    "        '''\n",
    "        # append memory\n",
    "        self.memory.append(event)\n",
    "\n",
    "        # Ensure memory not exceeding capacity\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Memory Sampling\n",
    "        '''\n",
    "        experiences = random.sample(self.memory, k=batch_size)\n",
    "\n",
    "        # Extract all elements\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float()\\\n",
    "            .to(self.device) # move to gpu\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long()\\\n",
    "            .to(self.device) # move to gpu\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float()\\\n",
    "            .to(self.device) # move to gpu\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float()\\\n",
    "            .to(self.device) # move to gpu\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float()\\\n",
    "            .to(self.device) # move to gpu\n",
    "        return states, next_states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.local_qnetwork = Network(state_size, action_size).to(self.device)\n",
    "        self.target_qnetwork = Network(state_size, action_size).to(self.device)\n",
    "        self.optimiser = optim.Adam(self.local_qnetwork.parameters(), lr=learning_rate)\n",
    "        self.memory = ReplayMemory(replay_buffer_size)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        Store and learn from Experiences\n",
    "        '''\n",
    "        self.memory.push((state, action, reward, next_state, done))\n",
    "        self.t_step = (self.t_step + 1) % 4\n",
    "\n",
    "        # Check every 4 steps\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory.memory) > minibatch_size:\n",
    "                experiences = self.memory.sample(100)\n",
    "                self.learn(experiences, discount_factor) # learn method will be created later\n",
    "    \n",
    "    def act(self, state, epsilon=.0):\n",
    "        '''\n",
    "        State processing to Policy implementation\n",
    "        '''\n",
    "        # unsqueeze: add an extra dimension to state vetor. Highly important to Deep Q-Learning\n",
    "        state = torch.from_numpy(state).float()\\\n",
    "            .unsqueeze(0)\\\n",
    "            .to(self.device)\n",
    "        self.local_qnetwork.eval() # eval() is a method inherited from the Network class\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action_values = self.local_qnetwork(state)\n",
    "\n",
    "        self.local_qnetwork.train()\n",
    "\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "    \n",
    "    def learn(self, experiences, discount_factor):\n",
    "        '''\n",
    "        Update the Agent's q-value based on sampled experiences\n",
    "        '''\n",
    "        states, next_states, actions, rewards, dones = experiences\n",
    "        next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + (discount_factor * next_q_targets * (1 - dones))\n",
    "        q_expected = self.local_qnetwork(states).gather(1, actions)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "        self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_param)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, interpolation_param):\n",
    "        for p_target, p_local in zip(target_model.parameters(), local_model.parameters()):\n",
    "            p_target.data.copy_(interpolation_param * p_local + (1.0 - interpolation_param) * p_target.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALISATION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
